{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "classification_CC22.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfLOnT3CoFrf"
      },
      "source": [
        "**Imports**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05gcTZ7GoLMY"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import glob\n",
        "import time\n",
        "import yaml as yl\n",
        "import pickle as pk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import subprocess\n",
        "import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from google.colab import drive\n",
        "from sklearn import model_selection\n",
        "from sklearn.metrics import confusion_matrix, classification_report"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DaWEOTYIuWYB"
      },
      "source": [
        "**Measure the initial time**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWyTW2aQuazz"
      },
      "source": [
        "initial_time = time.time()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FA-arG-UsiH"
      },
      "source": [
        "**Stellar** **Graph**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VLdG5pEUTWI"
      },
      "source": [
        "%pip install -q stellargraph==1.2.1\n",
        "\n",
        "import stellargraph as sg\n",
        "\n",
        "from stellargraph.mapper import PaddedGraphGenerator\n",
        "from stellargraph.layer import DeepGraphCNN\n",
        "from stellargraph import StellarDiGraph"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsKRcrjGoO8_"
      },
      "source": [
        "**Tensorflow**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbhHTBIlpmkE"
      },
      "source": [
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import Dense, Conv2D, Conv1D, MaxPool2D, MaxPool1D, Dropout, Flatten\n",
        "from tensorflow.keras.losses import categorical_crossentropy\n",
        "from tensorflow.keras.callbacks import EarlyStopping"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mhv8cqZgp6cM"
      },
      "source": [
        "**Profiler**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h58BOcwIp8iv",
        "outputId": "f2741811-f82f-4e6d-9ae1-7948c4b259c1"
      },
      "source": [
        "#%pip install line_profiler\n",
        "%pip install memory_profiler\n",
        "%pip install wandb\n",
        "\n",
        "#import line_profiler\n",
        "%load_ext memory_profiler\n",
        "import wandb"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: memory_profiler in /usr/local/lib/python3.7/dist-packages (0.58.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from memory_profiler) (5.4.8)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.7/dist-packages (0.12.4)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: yaspin>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.1.0)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.4.3)\n",
            "Requirement already satisfied: subprocess32>=3.5.3 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.5.4)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.7/dist-packages (from wandb) (0.1.2)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Requirement already satisfied: configparser>=3.8.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.2)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.1.24)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.0.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.0.7)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (3.7.4.3)\n",
            "Requirement already satisfied: smmap<5,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: termcolor<2.0.0,>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from yaspin>=1.0.0->wandb) (1.1.0)\n",
            "The memory_profiler extension is already loaded. To reload it, use:\n",
            "  %reload_ext memory_profiler\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_iCBIg3LqF9j"
      },
      "source": [
        "**Functions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zt0pBim4J5Rj"
      },
      "source": [
        "def execute(cmdline):\n",
        "  \"\"\"Execute a command.\"\"\"\n",
        "  try:\n",
        "    ret = subprocess.run(cmdline,\n",
        "                        shell=True,\n",
        "                        check=True,\n",
        "                        capture_output=True)\n",
        "  except subprocess.CalledProcessError:\n",
        "    print('Execute: {}'.format(cmdline))\n",
        "    return ''\n",
        "\n",
        "  return ret.stdout.decode(), ret.stderr.decode()\n",
        "\n",
        "def save(filename, data):\n",
        "  \"\"\"Save a file.\"\"\"\n",
        "  fout = open(filename, 'w')\n",
        "  fout.write(data)\n",
        "  fout.close()\n",
        "\n",
        "def random_features(sequence):\n",
        "  \"\"\"Create random features.\"\"\"\n",
        "  new_sequence = np.random.random(sequence.shape)\n",
        "  return new_sequence\n",
        "\n",
        "def zero_to_random_features(sequence):\n",
        "  \"\"\"\"Transform 0 in a random number.\"\"\"\n",
        "  new_sequence = np.copy(sequence)\n",
        "  new_sequence[new_sequence == 0] = np.random.random()\n",
        "  return new_sequence\n",
        "\n",
        "def one_to_random_features(sequence):\n",
        "  \"\"\"\"Transform 1 in a random number.\"\"\"\n",
        "  new_sequence = np.copy(sequence)\n",
        "  new_sequence[new_sequence == 1] = np.random.random()\n",
        "  return new_sequence\n",
        "\n",
        "\n",
        "def get_edges_dataFrame(edges, edge_type='original'):\n",
        "  \"\"\"Return the edges.\"\"\"\n",
        "  source = []\n",
        "  target = []\n",
        "  type_ = []\n",
        "  for node1, node2, data in edges:\n",
        "      source.append(node1)\n",
        "      target.append(node2)\n",
        "      if edge_type == 'original':\n",
        "          type_.append(data)\n",
        "      elif edge_type == 'default':\n",
        "          type_.append('default')\n",
        "      elif edge_type == 'random':\n",
        "          type_.append(np.random.random())\n",
        "      else:\n",
        "          print('Error: edge_type', edge_type)\n",
        "          sys.exit(1)\n",
        "\n",
        "  return pd.DataFrame({'source': source, 'target': target, 'type': type_})\n",
        "\n",
        "def stellar_graph_no_edge_features(graph):\n",
        "  \"\"\"Remove the type of the edges: <type> -> default.\"\"\"\n",
        "\n",
        "  edges = get_edges_dataFrame(graph.edges(1), 'default')\n",
        "\n",
        "  s_graph = StellarDiGraph(graph.node_features(),\n",
        "                           edges=edges,\n",
        "                           edge_type_column=\"type\")\n",
        "  return s_graph\n",
        "\n",
        "def stellar_graph_no_node_features(graph):\n",
        "  \"\"\"Remove the node features.\"\"\"\n",
        "\n",
        "  edges = get_edges_dataFrame(graph.edges(1), 'original')\n",
        "\n",
        "  nof_nodes = graph.number_of_nodes()\n",
        "  nof_features = graph.node_feature_shapes()['default'][0]\n",
        "\n",
        "  node_features = np.zeros((nof_nodes, nof_features))\n",
        "\n",
        "  s_graph = StellarDiGraph(node_features,\n",
        "                           edges=edges,\n",
        "                           edge_type_column=\"type\")\n",
        "  return s_graph\n",
        "\n",
        "def stellar_graph_random_edge_features(graph):\n",
        "  \"\"\"Add random edge features.\"\"\"\n",
        "\n",
        "  edges = get_edges_dataFrame(graph.edges(1), 'random')\n",
        "\n",
        "  s_graph = StellarDiGraph(graph.node_features(),\n",
        "                           edges=edges,\n",
        "                           edge_type_column=\"type\")\n",
        "  return s_graph\n",
        "\n",
        "def stellar_graph_random_node_features(graph):\n",
        "  \"\"\"Add random node features.\"\"\"\n",
        "\n",
        "  edges = get_edges_dataFrame(graph.edges(1), 'original')\n",
        "\n",
        "  nof_nodes = graph.number_of_nodes()\n",
        "  nof_features = graph.node_feature_shapes()['default'][0]\n",
        "\n",
        "  node_features = np.random.random((nof_nodes, nof_features))\n",
        "\n",
        "  s_graph = StellarDiGraph(node_features,\n",
        "                           edges=edges,\n",
        "                           edge_type_column=\"type\")\n",
        "  return s_graph\n",
        "\n",
        "def stellar_graph_no_features(graph):\n",
        "  \"\"\"Remove the egde/node features.\"\"\"\n",
        "\n",
        "  edges = get_edges_dataFrame(graph.edges(1), 'default')\n",
        "\n",
        "  nof_nodes = graph.number_of_nodes()\n",
        "  nof_features = graph.node_feature_shapes()['default'][0]\n",
        "\n",
        "  node_features = np.zeros((nof_nodes, nof_features))\n",
        "\n",
        "  s_graph = StellarDiGraph(node_features,\n",
        "                           edges=edges,\n",
        "                           edge_type_column=\"type\")\n",
        "  return s_graph\n",
        "\n",
        "def create_model_cnn(labels, input_shape, model_type='1d'):\n",
        "  \"\"\"Create the model.\"\"\"\n",
        "  layer_sizes = [3]\n",
        "  model = Sequential()\n",
        "\n",
        "  if model_type == '1d':\n",
        "    # padronizado com o GNN\n",
        "    model.add(Conv1D(filters=16, kernel_size=sum(layer_sizes), strides=sum(layer_sizes), activation=\"relu\", padding='same', input_shape=input_shape))\n",
        "    model.add(MaxPool1D(pool_size=2, strides=2))\n",
        "    model.add(Conv1D(filters=32, kernel_size=5, strides=1, activation=\"relu\"))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(units=128, activation=\"relu\"))\n",
        "    model.add(Dropout(rate=0.5))\n",
        "    model.add(Dense(units=labels, activation = 'softmax'))\n",
        "  elif model_type == '2d':\n",
        "    # modelo 1\n",
        "    # padronizado com o GNN\n",
        "    model.add(Conv2D(filters=16, kernel_size=(sum(layer_sizes), sum(layer_sizes)), strides=(sum(layer_sizes), sum(layer_sizes)), activation=\"relu\", padding='same', input_shape=input_shape))\n",
        "    model.add(MaxPool2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "    model.add(Conv2D(filters=32, kernel_size=(5, 5), strides=(1, 1), activation=\"relu\"))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(units=128, activation=\"relu\"))\n",
        "    model.add(Dropout(rate=0.5))\n",
        "    model.add(Dense(units=labels, activation = 'softmax'))\n",
        "  else:\n",
        "    print('Model type error')\n",
        "    sys.exit(1)\n",
        "\n",
        "  return model\n",
        "\n",
        "def create_model_lstm(labels, maxlen: int, embedding_dim: int, num_classes: int, dense_layer_size: int):\n",
        "\n",
        "  # Keras model\n",
        "  inp = Input(shape=(maxlen, embedding_dim,), dtype=\"float32\", name=\"code_in\")\n",
        "  x = LSTM(embedding_dim, implementation=1, return_sequences=True, name=\"lstm_1\")(inp)\n",
        "  x = LSTM(embedding_dim, implementation=1, name=\"lstm_2\")(x)\n",
        "\n",
        "  # Heuristic model: outputs 1-of-num_classes prediction\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Dense(dense_layer_size, activation=\"relu\")(x)\n",
        "  outputs = Dense(labels, activation=\"sigmoid\")(x)\n",
        "\n",
        "  return Model(inputs=inp, outputs=outputs)\n",
        "\n",
        "  self.model.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    metrics=['accuracy'])\n",
        "\n",
        "\n",
        "def create_model_gnn(labels, graphs):\n",
        "  generator = PaddedGraphGenerator(graphs=graphs)\n",
        "\n",
        "  k = 35  # the number of rows for the output tensor\n",
        "  layer_sizes = [32, 32, 32, 1]\n",
        "\n",
        "  dgcnn_model = DeepGraphCNN(\n",
        "    layer_sizes=layer_sizes,\n",
        "    activations=[\"tanh\", \"tanh\", \"tanh\", \"tanh\"],\n",
        "    k=k,\n",
        "    bias=False,\n",
        "    generator=generator,  \n",
        "  )\n",
        "  x_inp, x_out = dgcnn_model.in_out_tensors()\n",
        "  \n",
        "  # Add the convolutional, max pooling, and dense layers\n",
        "  x_out = Conv1D(filters=16, kernel_size=sum(layer_sizes), strides=sum(layer_sizes), activation=\"relu\")(x_out)\n",
        "  x_out = MaxPool1D(pool_size=2, strides=2)(x_out)\n",
        "  x_out = Conv1D(filters=32, kernel_size=5, strides=1, activation=\"relu\")(x_out)\n",
        "  x_out = Flatten()(x_out)\n",
        "  x_out = Dense(units=128, activation=\"relu\")(x_out)\n",
        "  x_out = Dropout(rate=0.5)(x_out)\n",
        "  outputs = Dense(units=labels, activation=\"softmax\")(x_out)\n",
        "  \n",
        "  #\n",
        "  # Create the model and prepare it for training by specifying the loss and optimization algorithm\n",
        "  #\n",
        "  model = Model(inputs=x_inp, outputs=outputs)\n",
        "\n",
        "  return model\n",
        "\n",
        "def load_desc_train_val_test_dataset_sequence(dataset_type,\n",
        "                                              compiler_level,\n",
        "                                              description_dataset_filename,\n",
        "                                              features):\n",
        "  \n",
        "  filename = '{}.yaml'.format(description_dataset_filename)\n",
        "  \n",
        "  wget = 'wget www.csl.uem.br/repository/data/datasets/{}'.format(filename)\n",
        "  !$wget \n",
        "  \n",
        "  fin = open(filename, 'r')\n",
        "  description_dataset = yl.load(fin)\n",
        "  fin.close()\n",
        "\n",
        "  for dataset, _ in description_dataset.items():\n",
        "    !mkdir $dataset\n",
        "    wget = 'wget www.csl.uem.br/repository/data/{}/{}/{}.tar.xz'.format(dataset, compiler_level, dataset_type)\n",
        "    tar = 'tar xfJ {}.tar.xz -C {}'.format(dataset_type, dataset)\n",
        "    rm = 'rm -rf {}*'.format(dataset_type)\n",
        "    !$wget\n",
        "    !$tar\n",
        "    !$rm\n",
        "  \n",
        "  X = {'training': [], 'validation': [], 'test': []}\n",
        "  Y = {'training': [], 'validation': [], 'test': []} \n",
        "\n",
        "  labels = []\n",
        "\n",
        "  for phase in ['training', 'validation', 'test']:\n",
        "\n",
        "    for dataset, dataset_data in description_dataset.items():\n",
        "\n",
        "      if phase not in dataset_data:\n",
        "        continue\n",
        "\n",
        "      for label, samples in dataset_data[phase].items():\n",
        "\n",
        "        int_label = int(label)\n",
        "        \n",
        "        # if dataset_type == 'prog2image' and int_label > 40:\n",
        "        if int_label > FLAGS_max_labels:\n",
        "          continue\n",
        "        \n",
        "        labels.append(int_label-1)\n",
        "\n",
        "        dataset_directory = os.path.join(dataset, dataset_type, label)\n",
        "\n",
        "        for sample in samples:\n",
        "          \n",
        "          try:\n",
        "            representation = np.load('{}/{}.npz'.format(dataset_directory, sample))\n",
        "          except:\n",
        "            print('Erro load', dataset_directory, sample, flush=True)\n",
        "            continue\n",
        "\n",
        "          if FLAGS_dataset_type in ['prog2image']:\n",
        "            representation = np.array(representation['values'], np.int8)\n",
        "          else:\n",
        "            representation = representation['values']\n",
        "          if features == 'random_features':\n",
        "            representation = random_features(representation)\n",
        "          elif features == 'zero_to_random_features':\n",
        "            representation = zero_to_random_features(representation)\n",
        "          elif features == 'one_to_random_features':\n",
        "            representation = one_to_random_features(representation)\n",
        "\n",
        "          Y[phase].append(int_label-1)\n",
        "          X[phase].append(representation)\n",
        "\n",
        "  labels = list(dict.fromkeys(labels))\n",
        "  \n",
        "  datasets = '* '.join(list(description_dataset.keys()))\n",
        "  rm = 'rm -rf {}* {}*'.format(datasets, description_dataset_filename)\n",
        "  !$rm\n",
        "\n",
        "  return X['training'], Y['training'], X['validation'], Y['validation'], X['test'], Y['test'], len(labels)\n",
        "\n",
        "def load_desc_train_val_test_dataset_graph(dataset_type,\n",
        "                                           embeddings,\n",
        "                                           compiler_level,\n",
        "                                           description_dataset_filename,\n",
        "                                           features):\n",
        "  \n",
        "  filename = '{}.yaml'.format(description_dataset_filename)\n",
        "  \n",
        "  wget = 'wget www.csl.uem.br/repository/data/datasets/{}'.format(filename)\n",
        "  !$wget \n",
        "  \n",
        "  fin = open(filename, 'r')\n",
        "  description_dataset = yl.load(fin)\n",
        "  fin.close()\n",
        "\n",
        "  for dataset, _ in description_dataset.items():\n",
        "    !mkdir $dataset\n",
        "    wget = 'wget www.csl.uem.br/repository/data/{}/{}/{}.tar.xz'.format(dataset, compiler_level, dataset_type)\n",
        "    tar = 'tar xfJ {}.tar.xz -C {}'.format(dataset_type, dataset)\n",
        "    rm = 'rm -rf {}*'.format(dataset_type)\n",
        "    !$wget\n",
        "    !$tar\n",
        "    !$rm\n",
        "\n",
        "  X = []\n",
        "  Y = {'training': [], 'validation': [], 'test': []} \n",
        "  Y_index = {'training': [], 'validation': [], 'test': []} \n",
        "\n",
        "  labels = []\n",
        "\n",
        "  for phase in ['training', 'validation', 'test']:\n",
        "\n",
        "    for dataset, dataset_data in description_dataset.items():\n",
        "\n",
        "      if phase not in dataset_data:\n",
        "        continue\n",
        "\n",
        "      for label, samples in dataset_data[phase].items():\n",
        "\n",
        "        int_label = int(label)\n",
        "\n",
        "        labels.append(int_label-1)\n",
        "\n",
        "        dataset_directory = os.path.join(dataset, dataset_type, embeddings, label)\n",
        "\n",
        "        for sample in samples:\n",
        "\n",
        "          try:\n",
        "            filename = '{}/{}.pk'.format(dataset_directory, sample)\n",
        "            fin = open(filename, 'rb')\n",
        "            representation = pk.load(fin)\n",
        "            fin.close()                  \n",
        "          except:\n",
        "            print('Erro load', dataset_directory, sample, flush=True)\n",
        "            continue\n",
        "\n",
        "          if features == 'no_edge_features':\n",
        "            representation = stellar_graph_no_edge_features(representation)\n",
        "          elif features == 'no_node_features':\n",
        "            representation = stellar_graph_no_node_features(representation)\n",
        "          elif features == 'random_edge_features':\n",
        "            representation = stellar_graph_random_edge_features(representation)        \n",
        "          elif features == 'random_node_features':\n",
        "            representation = stellar_graph_random_node_features(representation)       \n",
        "          elif features == 'no_features':\n",
        "            representation = stellar_graph_no_features(representation)\n",
        "\n",
        "          Y[phase].append(int_label-1)\n",
        "          Y_index[phase].append(len(X))\n",
        "          X.append(representation)\n",
        "\n",
        "  labels = list(dict.fromkeys(labels))\n",
        "  \n",
        "  datasets = '* '.join(list(description_dataset.keys()))\n",
        "  rm = 'rm -rf {}* {}*'.format(datasets, description_dataset_filename)\n",
        "  !$rm\n",
        "  \n",
        "  Y_train = pd.Series(Y['training'], index=Y_index['training'], name='label', dtype=\"category\")  \n",
        "  Y_val = pd.Series(Y['validation'], index=Y_index['validation'], name='label', dtype=\"category\")  \n",
        "  Y_test = pd.Series(Y['test'], index=Y_index['test'], name='label', dtype=\"category\")\n",
        "  \n",
        "  Y_train = pd.get_dummies(Y_train)\n",
        "  Y_val = pd.get_dummies(Y_val)\n",
        "  Y_test = pd.get_dummies(Y_test)\n",
        "\n",
        "  return X, Y_train, Y_val, Y_test, len(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9MTLD1djY3l"
      },
      "source": [
        "**Arguments**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4AqVescjcvB"
      },
      "source": [
        "#\n",
        "# TRAIN, VALIDATION AND TEST RATIOS\n",
        "#\n",
        "\n",
        "# Train data ratio\n",
        "FLAGS_train_ratio = 0.75\n",
        "\n",
        "# Validation data ratio\n",
        "FLAGS_val_ratio = 0.25\n",
        "\n",
        "# Test data ratio\n",
        "FLAGS_test_ratio = 0.20\n",
        "\n",
        "#\n",
        "# MODEL\n",
        "#\n",
        "\n",
        "# Number of epochs\n",
        "#\n",
        "# Zhang = 100\n",
        "# Brauckmann = 1000\n",
        "# Cummins = 300\n",
        "#\n",
        "FLAGS_epochs = 200\n",
        "\n",
        "# Patience\n",
        "FLAGS_patience = 20\n",
        "\n",
        "# Verbose\n",
        "FLAGS_training_verbose = 1\n",
        "\n",
        "#\n",
        "# GOOGLE DRIVER\n",
        "#\n",
        "FLAGS_driver = '/content/drive'\n",
        "\n",
        "#\n",
        "# CONTROLS\n",
        "#\n",
        "\n",
        "# Store the results?\n",
        "FLAGS_store_results = False\n",
        "\n",
        "# Store the model?\n",
        "FLAGS_store_model = False\n",
        "\n",
        "# Store Wandb data?\n",
        "FLAGS_store_wandb = False\n",
        "\n",
        "#\n",
        "# DATASET\n",
        "#\n",
        "\n",
        "# MAX POJ DATASET\n",
        "FLAGS_max_labels = 50\n",
        "\n",
        "# Compiler level\n",
        "#\n",
        "# - O0 (the compiler level enabled during data generation)\n",
        "# - O0_50C -> POJ 50 classes\n",
        "# - Oz\n",
        "FLAGS_compiler_level = 'O0_50C'\n",
        "\n",
        "# Type\n",
        "#\n",
        "# Sequences (vector, matrix)\n",
        "# - inst2vec.lower\n",
        "# - inst2vec.2d (2d)\n",
        "# - ir2vec.program\n",
        "# - milepost\n",
        "# - llvm_histogram\n",
        "# - lbp\n",
        "# - rbp\n",
        "# - prog2image (2d)\n",
        "#\n",
        "# Graph\n",
        "# - cfgcallnoroot\n",
        "# - cfgcallcompactnoroot\n",
        "# - cfgcallcompact1enoroot\n",
        "# - cdfgcallnoroot\n",
        "# - cdfgcallcompactnoroot\n",
        "# - cdfgcallcompact1enoroot\n",
        "# - cdfgplusnorrot\n",
        "# - programlnoroot\n",
        "#\n",
        "FLAGS_dataset_type = 'prog2image'\n",
        "\n",
        "# CNN or GNN?\n",
        "FLAGS_CNN_dataset_types = ['inst2vec.lower',\n",
        "                           'inst2vec.2d',\n",
        "                           'ir2vec.program',\n",
        "                           'milepost',\n",
        "                           'llvm_histogram',\n",
        "                           'lbp',\n",
        "                           'rbp',\n",
        "                           'prog2image']\n",
        "\n",
        "# CNN 1d or CNN 2d?\n",
        "FLAGS_CNN_2d_dataset_types = ['inst2vec.2d', 'prog2image']\n",
        "\n",
        "# Embeddings\n",
        "#\n",
        "# bag_of_words\n",
        "# inst2vec\n",
        "# ir2vec\n",
        "# opcode\n",
        "# word2vec\n",
        "FLAGS_embeddings = 'ir2vec' \n",
        "\n",
        "# Features type: random, zero to random, one to random\n",
        "FLAGS_features = 'all_features'\n",
        "\n",
        "# Round\n",
        "FLAGS_round = 'round3'\n",
        "\n",
        "# Description dataset filename\n",
        "#\n",
        "# AnghaBestSeqsSBLP2021_500_inst2vec_TVT.yaml // não existem 1000 exemplos para inst2vec\n",
        "# AnghaBestSeqsSBLP2021_1000.TVT.yaml // 1000 por classe\n",
        "# CodeNetBestSeqsSBLP2021_TVT.yaml // 500 por classe\n",
        "# AnghaLoops_TVT.yaml // 300 por classe\n",
        "# POJ_TVT.yaml // 500 por classe\n",
        "# POJ50C_TVT.yaml // 500 por 50 classes\n",
        "#\n",
        "FLAGS_description_dataset_filename = 'POJ50C_TVT'\n",
        "\n",
        "#\n",
        "# RESULTS\n",
        "#\n",
        "\n",
        "# Top directory\n",
        "FLAGS_top_directory = 'My Drive/CC22'\n",
        "\n",
        "# Result directory suffix\n",
        "FLAGS_suffix = ''\n",
        "\n",
        "# Results directory\n",
        "if FLAGS_dataset_type in FLAGS_CNN_dataset_types:\n",
        "  last = '{}_{}'.format(FLAGS_features,\n",
        "                        FLAGS_compiler_level) if not FLAGS_suffix else '{}_{}_{}'.format(FLAGS_features,\n",
        "                                                                                         FLAGS_compiler_level,\n",
        "                                                                                         FLAGS_suffix)\n",
        "else:\n",
        "    last = '{}_{}_{}'.format(FLAGS_features,\n",
        "                             FLAGS_embeddings,\n",
        "                             FLAGS_compiler_level) if not FLAGS_suffix else '{}_{}_{}_{}'.format(FLAGS_features,\n",
        "                                                                                                 FLAGS_embeddings,\n",
        "                                                                                                 FLAGS_compiler_level,\n",
        "                                                                                                 FLAGS_suffix)\n",
        " \n",
        "FLAGS_results_directory = os.path.join(FLAGS_driver,\n",
        "                                       FLAGS_top_directory,\n",
        "                                       FLAGS_description_dataset_filename,\n",
        "                                       FLAGS_round,\n",
        "                                       FLAGS_dataset_type,\n",
        "                                       last)\n",
        "\n",
        "# Wandb project name\n",
        "FLAGS_project_name = 'CC22'\n",
        "\n",
        "# Breakdown the runtime\n",
        "FLAGS_times = {}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIvRzT4mqI3e"
      },
      "source": [
        "**Open Google Drive**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dt6MwMNNqNXP"
      },
      "source": [
        "if FLAGS_store_results:\n",
        "  drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xccaNp8obm1a"
      },
      "source": [
        "**Initiate Wandb**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSyfMH6ebqAt"
      },
      "source": [
        "if FLAGS_store_wandb:\n",
        "  PROJECT_NAME = '{}_{}_{}_{}_{}'.format(FLAGS_project_name,\n",
        "                                         FLAGS_description_dataset_filename,\n",
        "                                         FLAGS_round,\n",
        "                                         FLAGS_dataset_type,\n",
        "                                         last)\n",
        "  wandb.init(project=PROJECT_NAME, entity='andersonfaustino')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XwBQyf7auFe"
      },
      "source": [
        "**Load the dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8k6aYBimBlr",
        "outputId": "22effefa-71b2-43f7-b609-094ecdbb82a0"
      },
      "source": [
        "start = time.time()\n",
        "\n",
        "if FLAGS_dataset_type in FLAGS_CNN_dataset_types:\n",
        "  # dados estão em diretórios reparados\n",
        "  #\n",
        "  #  training:\n",
        "  #    1: \n",
        "  #    2:\n",
        "  #    ...\n",
        "  #  validation:\n",
        "  #     ...\n",
        "  #  test:\n",
        "  #     ...\n",
        "  #\n",
        "  X_train, Y_train, X_val, Y_val, X_test, Y_test, FLAGS_labels = load_desc_train_val_test_dataset_sequence(FLAGS_dataset_type,\n",
        "                                                                                                           FLAGS_compiler_level,\n",
        "                                                                                                           FLAGS_description_dataset_filename,\n",
        "                                                                                                           FLAGS_features)\n",
        "\n",
        "else:\n",
        "  # dados estão em diretórios reparados\n",
        "  #\n",
        "  #  training:\n",
        "  #    1: \n",
        "  #    2:\n",
        "  #    ...\n",
        "  #  validation:\n",
        "  #     ...\n",
        "  #  test:\n",
        "  #     ...\n",
        "  #\n",
        "  X, Y_train, Y_val, Y_test, FLAGS_labels = load_desc_train_val_test_dataset_graph(FLAGS_dataset_type,\n",
        "                                                                                   FLAGS_embeddings,\n",
        "                                                                                   FLAGS_compiler_level,\n",
        "                                                                                   FLAGS_description_dataset_filename,\n",
        "                                                                                   FLAGS_features)\n",
        "\n",
        "end = time.time()\n",
        "\n",
        "FLAGS_times['loading'] = end - start"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2021-10-06 15:04:55--  http://www.csl.uem.br/repository/data/datasets/POJ50C_TVT.yaml\n",
            "Resolving www.csl.uem.br (www.csl.uem.br)... 186.233.153.209\n",
            "Connecting to www.csl.uem.br (www.csl.uem.br)|186.233.153.209|:80... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://www.csl.uem.br/repository/data/datasets/POJ50C_TVT.yaml [following]\n",
            "--2021-10-06 15:04:56--  https://www.csl.uem.br/repository/data/datasets/POJ50C_TVT.yaml\n",
            "Connecting to www.csl.uem.br (www.csl.uem.br)|186.233.153.209|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 312831 (305K) [application/octet-stream]\n",
            "Saving to: ‘POJ50C_TVT.yaml’\n",
            "\n",
            "POJ50C_TVT.yaml     100%[===================>] 305.50K   345KB/s    in 0.9s    \n",
            "\n",
            "2021-10-06 15:04:58 (345 KB/s) - ‘POJ50C_TVT.yaml’ saved [312831/312831]\n",
            "\n",
            "--2021-10-06 15:05:00--  http://www.csl.uem.br/repository/data/POJ/O0_50C/prog2image.tar.xz\n",
            "Resolving www.csl.uem.br (www.csl.uem.br)... 186.233.153.209\n",
            "Connecting to www.csl.uem.br (www.csl.uem.br)|186.233.153.209|:80... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://www.csl.uem.br/repository/data/POJ/O0_50C/prog2image.tar.xz [following]\n",
            "--2021-10-06 15:05:02--  https://www.csl.uem.br/repository/data/POJ/O0_50C/prog2image.tar.xz\n",
            "Connecting to www.csl.uem.br (www.csl.uem.br)|186.233.153.209|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 42382428 (40M) [application/octet-stream]\n",
            "Saving to: ‘prog2image.tar.xz’\n",
            "\n",
            "prog2image.tar.xz   100%[===================>]  40.42M  3.63MB/s    in 12s     \n",
            "\n",
            "2021-10-06 15:05:16 (3.26 MB/s) - ‘prog2image.tar.xz’ saved [42382428/42382428]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "woargju4WJ-J"
      },
      "source": [
        "**Prepare the dataset (CNN)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qigFBx3-lIDs",
        "outputId": "8bc09b4c-6e9d-4cd5-e74c-407bad5adee7"
      },
      "source": [
        "if FLAGS_dataset_type in FLAGS_CNN_dataset_types:\n",
        "  if FLAGS_dataset_type in ['prog2image']:\n",
        "    X_train = np.array(X_train, dtype=np.int8)\n",
        "    Y_train = np.array(Y_train, dtype=np.int8)\n",
        "    X_val = np.array(X_val, dtype=np.int8)\n",
        "    Y_val = np.array(Y_val, dtype=np.int8)\n",
        "    X_test = np.array(X_test, dtype=np.int8)\n",
        "    Y_test = np.array(Y_test, dtype=np.int8)\n",
        "  else:\n",
        "    X_train = np.array(X_train)\n",
        "    Y_train = np.array(Y_train)\n",
        "    X_val = np.array(X_val)\n",
        "    Y_val = np.array(Y_val)\n",
        "    X_test = np.array(X_test)\n",
        "    Y_test = np.array(Y_test)\n",
        "\n",
        "  # 1D Model\n",
        "  if FLAGS_dataset_type not in FLAGS_CNN_2d_dataset_types:\n",
        "    X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
        "    X_val = X_val.reshape(X_val.shape[0], X_val.shape[1], 1)\n",
        "    X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
        "  else:\n",
        "    # 2D Model\n",
        "    rows = X_train[0].shape[0]\n",
        "    cols = X_train[0].shape[1]\n",
        "    X_train = X_train.reshape(X_train.shape[0], rows, cols, 1)\n",
        "    X_val = X_val.reshape(X_val.shape[0], rows, cols, 1)\n",
        "    X_test = X_test.reshape(X_test.shape[0], rows, cols, 1)\n",
        "\n",
        "  print('Training:', X_train.shape[0], flush=True)\n",
        "  print('Validation:', X_val.shape[0], flush=True)\n",
        "  print('Test:', X_test.shape[0], flush=True)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training: 15000\n",
            "Validation: 5000\n",
            "Test: 5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mINEpFYdEwmd"
      },
      "source": [
        "**Prepare the dataset (GNN)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qiZcIqKSZnRs"
      },
      "source": [
        "if FLAGS_dataset_type not in FLAGS_CNN_dataset_types:\n",
        "  %memit gen = PaddedGraphGenerator(graphs=X)\n",
        "\n",
        "  %memit train_gen = gen.flow(list(Y_train.index), targets=Y_train.values, batch_size=50, symmetric_normalization=False)\n",
        "  %memit val_gen = gen.flow(list(Y_val.index), targets=Y_val.values, batch_size=1, symmetric_normalization=False)\n",
        "  %memit test_gen = gen.flow(list(Y_test.index), targets=Y_test.values, batch_size=1, symmetric_normalization=False) \n",
        "\n",
        "  print('Training:', len(Y_train), flush=True)\n",
        "  print('Validation:', len(Y_val), flush=True)\n",
        "  print('Test:', len(Y_test), flush=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-61I4RgZ5gj"
      },
      "source": [
        "**Create the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Co-j9Rj4Z707",
        "outputId": "a5514bb3-a847-4f59-a720-99f1eb751a8f"
      },
      "source": [
        "if FLAGS_dataset_type in FLAGS_CNN_dataset_types:\n",
        "  input_shape = X_train[0].shape\n",
        "  model_type = '1d' if FLAGS_dataset_type not in FLAGS_CNN_2d_dataset_types else '2d'\n",
        "\n",
        "  model = create_model_cnn(FLAGS_labels, input_shape, model_type)\n",
        "  model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(learning_rate=0.0001), metrics=['accuracy'])\n",
        "  model.summary()\n",
        "else:\n",
        "  model = create_model_gnn(FLAGS_labels, X)\n",
        "  model.compile(optimizer=Adam(learning_rate=0.0001), loss=categorical_crossentropy, metrics=['accuracy'])\n",
        "  model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 84, 86, 16)        160       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 42, 43, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 38, 39, 32)        12832     \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 47424)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               6070400   \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 50)                6450      \n",
            "=================================================================\n",
            "Total params: 6,089,842\n",
            "Trainable params: 6,089,842\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9zn2-s6V681"
      },
      "source": [
        "**Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "udFCunxrV-qK",
        "outputId": "89d59fb9-6a4f-4a29-d8ce-fcd0959ab34a"
      },
      "source": [
        "es_callback = EarlyStopping(monitor=\"val_accuracy\", patience=FLAGS_patience, restore_best_weights=True)\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "if FLAGS_dataset_type in FLAGS_CNN_dataset_types:\n",
        "  %memit history = model.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=FLAGS_epochs, verbose=FLAGS_training_verbose, shuffle=True, callbacks=[es_callback])\n",
        "else:  \n",
        "  %memit history = model.fit(train_gen, validation_data=val_gen, epochs=FLAGS_epochs, verbose=FLAGS_training_verbose, shuffle=True, callbacks=[es_callback])\n",
        "\n",
        "end = time.time()\n",
        "\n",
        "FLAGS_times['training'] = end - start"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "469/469 [==============================] - 20s 6ms/step - loss: 3.7795 - accuracy: 0.0573 - val_loss: 3.5462 - val_accuracy: 0.1678\n",
            "Epoch 2/200\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 3.4889 - accuracy: 0.1283 - val_loss: 3.2159 - val_accuracy: 0.2784\n",
            "Epoch 3/200\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 3.2654 - accuracy: 0.1775 - val_loss: 2.9476 - val_accuracy: 0.3214\n",
            "Epoch 4/200\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 3.0956 - accuracy: 0.2107 - val_loss: 2.7370 - val_accuracy: 0.3606\n",
            "Epoch 5/200\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 2.9495 - accuracy: 0.2384 - val_loss: 2.6006 - val_accuracy: 0.3856\n",
            "Epoch 6/200\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 2.8356 - accuracy: 0.2630 - val_loss: 2.4992 - val_accuracy: 0.4118\n",
            "Epoch 7/200\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 2.7458 - accuracy: 0.2780 - val_loss: 2.4393 - val_accuracy: 0.4350\n",
            "Epoch 8/200\n",
            "469/469 [==============================] - 3s 6ms/step - loss: 2.6540 - accuracy: 0.2960 - val_loss: 2.3083 - val_accuracy: 0.4366\n",
            "Epoch 9/200\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 2.5874 - accuracy: 0.3068 - val_loss: 2.2948 - val_accuracy: 0.4540\n",
            "Epoch 10/200\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 2.5160 - accuracy: 0.3175 - val_loss: 2.2107 - val_accuracy: 0.4686\n",
            "Epoch 11/200\n",
            "469/469 [==============================] - 3s 6ms/step - loss: 2.4567 - accuracy: 0.3263 - val_loss: 2.1750 - val_accuracy: 0.4780\n",
            "Epoch 12/200\n",
            "469/469 [==============================] - 3s 6ms/step - loss: 2.3914 - accuracy: 0.3379 - val_loss: 2.1463 - val_accuracy: 0.4776\n",
            "Epoch 13/200\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 2.3267 - accuracy: 0.3567 - val_loss: 2.0494 - val_accuracy: 0.4794\n",
            "Epoch 14/200\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 2.2799 - accuracy: 0.3645 - val_loss: 2.0089 - val_accuracy: 0.4958\n",
            "Epoch 15/200\n",
            "469/469 [==============================] - 3s 6ms/step - loss: 2.2344 - accuracy: 0.3702 - val_loss: 1.9654 - val_accuracy: 0.5002\n",
            "Epoch 16/200\n",
            "469/469 [==============================] - 3s 6ms/step - loss: 2.1932 - accuracy: 0.3804 - val_loss: 1.9869 - val_accuracy: 0.5122\n",
            "Epoch 17/200\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 2.1350 - accuracy: 0.3926 - val_loss: 1.9299 - val_accuracy: 0.5034\n",
            "Epoch 18/200\n",
            "469/469 [==============================] - 3s 6ms/step - loss: 2.0756 - accuracy: 0.4027 - val_loss: 1.9009 - val_accuracy: 0.5180\n",
            "Epoch 19/200\n",
            "469/469 [==============================] - 3s 6ms/step - loss: 2.0700 - accuracy: 0.4092 - val_loss: 1.8697 - val_accuracy: 0.5198\n",
            "Epoch 20/200\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 2.0056 - accuracy: 0.4199 - val_loss: 1.8508 - val_accuracy: 0.5258\n",
            "Epoch 21/200\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 1.9616 - accuracy: 0.4253 - val_loss: 1.8187 - val_accuracy: 0.5278\n",
            "Epoch 22/200\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 1.9340 - accuracy: 0.4315 - val_loss: 1.8039 - val_accuracy: 0.5338\n",
            "Epoch 23/200\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 1.8988 - accuracy: 0.4391 - val_loss: 1.7833 - val_accuracy: 0.5338\n",
            "Epoch 24/200\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 1.8475 - accuracy: 0.4463 - val_loss: 1.8002 - val_accuracy: 0.5270\n",
            "Epoch 25/200\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 1.8262 - accuracy: 0.4565 - val_loss: 1.7636 - val_accuracy: 0.5456\n",
            "Epoch 26/200\n",
            "469/469 [==============================] - 3s 6ms/step - loss: 1.7819 - accuracy: 0.4659 - val_loss: 1.7667 - val_accuracy: 0.5432\n",
            "Epoch 27/200\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 1.7479 - accuracy: 0.4662 - val_loss: 1.7426 - val_accuracy: 0.5486\n",
            "Epoch 28/200\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 1.7194 - accuracy: 0.4763 - val_loss: 1.7442 - val_accuracy: 0.5454\n",
            "Epoch 29/200\n",
            "469/469 [==============================] - 3s 6ms/step - loss: 1.6874 - accuracy: 0.4836 - val_loss: 1.7451 - val_accuracy: 0.5514\n",
            "Epoch 30/200\n",
            "469/469 [==============================] - 3s 6ms/step - loss: 1.6687 - accuracy: 0.4872 - val_loss: 1.7547 - val_accuracy: 0.5460\n",
            "Epoch 31/200\n",
            "469/469 [==============================] - 3s 6ms/step - loss: 1.6405 - accuracy: 0.4913 - val_loss: 1.7572 - val_accuracy: 0.5478\n",
            "Epoch 32/200\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 1.6014 - accuracy: 0.5021 - val_loss: 1.7114 - val_accuracy: 0.5582\n",
            "Epoch 33/200\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 1.5787 - accuracy: 0.5065 - val_loss: 1.7223 - val_accuracy: 0.5610\n",
            "Epoch 34/200\n",
            "469/469 [==============================] - 3s 6ms/step - loss: 1.5610 - accuracy: 0.5043 - val_loss: 1.7126 - val_accuracy: 0.5558\n",
            "Epoch 35/200\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 1.5347 - accuracy: 0.5155 - val_loss: 1.7100 - val_accuracy: 0.5626\n",
            "Epoch 36/200\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 1.5169 - accuracy: 0.5225 - val_loss: 1.7104 - val_accuracy: 0.5640\n",
            "Epoch 37/200\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 1.4741 - accuracy: 0.5297 - val_loss: 1.6970 - val_accuracy: 0.5630\n",
            "Epoch 38/200\n",
            "469/469 [==============================] - 3s 6ms/step - loss: 1.4611 - accuracy: 0.5296 - val_loss: 1.7135 - val_accuracy: 0.5682\n",
            "Epoch 39/200\n",
            "469/469 [==============================] - 3s 6ms/step - loss: 1.4524 - accuracy: 0.5280 - val_loss: 1.7060 - val_accuracy: 0.5666\n",
            "Epoch 40/200\n",
            "469/469 [==============================] - 3s 6ms/step - loss: 1.4310 - accuracy: 0.5385 - val_loss: 1.7298 - val_accuracy: 0.5640\n",
            "Epoch 41/200\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 1.3987 - accuracy: 0.5486 - val_loss: 1.7233 - val_accuracy: 0.5730\n",
            "Epoch 42/200\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 1.3721 - accuracy: 0.5533 - val_loss: 1.7353 - val_accuracy: 0.5754\n",
            "Epoch 43/200\n",
            "469/469 [==============================] - 3s 6ms/step - loss: 1.3743 - accuracy: 0.5492 - val_loss: 1.7199 - val_accuracy: 0.5668\n",
            "Epoch 44/200\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 1.3489 - accuracy: 0.5589 - val_loss: 1.7665 - val_accuracy: 0.5732\n",
            "Epoch 45/200\n",
            "469/469 [==============================] - 3s 6ms/step - loss: 1.3408 - accuracy: 0.5608 - val_loss: 1.7462 - val_accuracy: 0.5712\n",
            "Epoch 46/200\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 1.3056 - accuracy: 0.5701 - val_loss: 1.7014 - val_accuracy: 0.5792\n",
            "Epoch 47/200\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 1.2989 - accuracy: 0.5735 - val_loss: 1.7255 - val_accuracy: 0.5744\n",
            "Epoch 48/200\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 1.2684 - accuracy: 0.5753 - val_loss: 1.7613 - val_accuracy: 0.5756\n",
            "Epoch 49/200\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 1.2638 - accuracy: 0.5793 - val_loss: 1.7468 - val_accuracy: 0.5780\n",
            "Epoch 50/200\n",
            "469/469 [==============================] - 3s 6ms/step - loss: 1.2500 - accuracy: 0.5862 - val_loss: 1.8053 - val_accuracy: 0.5756\n",
            "Epoch 51/200\n",
            "469/469 [==============================] - 3s 6ms/step - loss: 1.2406 - accuracy: 0.5847 - val_loss: 1.7467 - val_accuracy: 0.5768\n",
            "Epoch 52/200\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 1.2138 - accuracy: 0.5945 - val_loss: 1.8059 - val_accuracy: 0.5812\n",
            "Epoch 53/200\n",
            "469/469 [==============================] - 3s 6ms/step - loss: 1.1961 - accuracy: 0.5954 - val_loss: 1.7816 - val_accuracy: 0.5870\n",
            "Epoch 54/200\n",
            "469/469 [==============================] - 3s 6ms/step - loss: 1.2005 - accuracy: 0.5960 - val_loss: 1.7989 - val_accuracy: 0.5768\n",
            "Epoch 55/200\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 1.1793 - accuracy: 0.5992 - val_loss: 1.8342 - val_accuracy: 0.5746\n",
            "Epoch 56/200\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 1.1835 - accuracy: 0.5961 - val_loss: 1.7893 - val_accuracy: 0.5856\n",
            "Epoch 57/200\n",
            "469/469 [==============================] - 3s 6ms/step - loss: 1.1451 - accuracy: 0.6101 - val_loss: 1.8144 - val_accuracy: 0.5872\n",
            "Epoch 58/200\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 1.1569 - accuracy: 0.6059 - val_loss: 1.8239 - val_accuracy: 0.5890\n",
            "Epoch 59/200\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 1.1422 - accuracy: 0.6083 - val_loss: 1.8388 - val_accuracy: 0.5838\n",
            "Epoch 60/200\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 1.1284 - accuracy: 0.6127 - val_loss: 1.8699 - val_accuracy: 0.5864\n",
            "Epoch 61/200\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 1.1338 - accuracy: 0.6100 - val_loss: 1.8836 - val_accuracy: 0.5874\n",
            "Epoch 62/200\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 1.1078 - accuracy: 0.6143 - val_loss: 1.8433 - val_accuracy: 0.5884\n",
            "Epoch 63/200\n",
            "469/469 [==============================] - 3s 6ms/step - loss: 1.0676 - accuracy: 0.6306 - val_loss: 1.8821 - val_accuracy: 0.5914\n",
            "Epoch 64/200\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 1.0924 - accuracy: 0.6193 - val_loss: 1.8944 - val_accuracy: 0.5842\n",
            "Epoch 65/200\n",
            "469/469 [==============================] - 3s 5ms/step - loss: 1.0655 - accuracy: 0.6302 - val_loss: 1.9363 - val_accuracy: 0.5858\n",
            "Epoch 66/200\n",
            "469/469 [==============================] - 3s 6ms/step - loss: 1.0572 - accuracy: 0.6321 - val_loss: 1.9057 - val_accuracy: 0.5862\n",
            "Epoch 67/200\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 1.0537 - accuracy: 0.6303 - val_loss: 1.9385 - val_accuracy: 0.5872\n",
            "Epoch 68/200\n",
            "469/469 [==============================] - 3s 6ms/step - loss: 1.0286 - accuracy: 0.6443 - val_loss: 1.9416 - val_accuracy: 0.5866\n",
            "Epoch 69/200\n",
            "469/469 [==============================] - 3s 6ms/step - loss: 1.0230 - accuracy: 0.6403 - val_loss: 1.9432 - val_accuracy: 0.5944\n",
            "Epoch 70/200\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 1.0183 - accuracy: 0.6412 - val_loss: 1.9447 - val_accuracy: 0.5914\n",
            "Epoch 71/200\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 1.0153 - accuracy: 0.6415 - val_loss: 1.9716 - val_accuracy: 0.5878\n",
            "Epoch 72/200\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 1.0216 - accuracy: 0.6404 - val_loss: 1.9961 - val_accuracy: 0.5962\n",
            "Epoch 73/200\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.9962 - accuracy: 0.6487 - val_loss: 2.0104 - val_accuracy: 0.5918\n",
            "Epoch 74/200\n",
            "469/469 [==============================] - 3s 6ms/step - loss: 0.9917 - accuracy: 0.6526 - val_loss: 2.0219 - val_accuracy: 0.5948\n",
            "Epoch 75/200\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.9939 - accuracy: 0.6455 - val_loss: 1.9764 - val_accuracy: 0.5902\n",
            "Epoch 76/200\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.9701 - accuracy: 0.6587 - val_loss: 2.0695 - val_accuracy: 0.5874\n",
            "Epoch 77/200\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.9609 - accuracy: 0.6546 - val_loss: 2.0971 - val_accuracy: 0.6008\n",
            "Epoch 78/200\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.9549 - accuracy: 0.6609 - val_loss: 2.0934 - val_accuracy: 0.5904\n",
            "Epoch 79/200\n",
            "469/469 [==============================] - 3s 6ms/step - loss: 0.9636 - accuracy: 0.6562 - val_loss: 2.1262 - val_accuracy: 0.5940\n",
            "Epoch 80/200\n",
            "469/469 [==============================] - 3s 6ms/step - loss: 0.9452 - accuracy: 0.6645 - val_loss: 2.1089 - val_accuracy: 0.5932\n",
            "Epoch 81/200\n",
            "469/469 [==============================] - 3s 5ms/step - loss: 0.9332 - accuracy: 0.6669 - val_loss: 2.1573 - val_accuracy: 0.5966\n",
            "Epoch 82/200\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.9506 - accuracy: 0.6619 - val_loss: 2.0918 - val_accuracy: 0.5970\n",
            "Epoch 83/200\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.9435 - accuracy: 0.6612 - val_loss: 2.1184 - val_accuracy: 0.5934\n",
            "Epoch 84/200\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.9243 - accuracy: 0.6728 - val_loss: 2.0971 - val_accuracy: 0.6000\n",
            "Epoch 85/200\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.9224 - accuracy: 0.6695 - val_loss: 2.2062 - val_accuracy: 0.5946\n",
            "Epoch 86/200\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.9151 - accuracy: 0.6706 - val_loss: 2.2344 - val_accuracy: 0.6000\n",
            "Epoch 87/200\n",
            "469/469 [==============================] - 3s 6ms/step - loss: 0.9039 - accuracy: 0.6703 - val_loss: 2.1647 - val_accuracy: 0.5926\n",
            "Epoch 88/200\n",
            "469/469 [==============================] - 3s 6ms/step - loss: 0.9063 - accuracy: 0.6707 - val_loss: 2.1902 - val_accuracy: 0.5932\n",
            "Epoch 89/200\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.8872 - accuracy: 0.6774 - val_loss: 2.2018 - val_accuracy: 0.5976\n",
            "Epoch 90/200\n",
            "469/469 [==============================] - 3s 6ms/step - loss: 0.8944 - accuracy: 0.6777 - val_loss: 2.3010 - val_accuracy: 0.5940\n",
            "Epoch 91/200\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.9021 - accuracy: 0.6711 - val_loss: 2.2384 - val_accuracy: 0.5994\n",
            "Epoch 92/200\n",
            "469/469 [==============================] - 3s 6ms/step - loss: 0.8774 - accuracy: 0.6812 - val_loss: 2.2131 - val_accuracy: 0.5956\n",
            "Epoch 93/200\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.8724 - accuracy: 0.6859 - val_loss: 2.2705 - val_accuracy: 0.5984\n",
            "Epoch 94/200\n",
            "469/469 [==============================] - 3s 6ms/step - loss: 0.8801 - accuracy: 0.6803 - val_loss: 2.4625 - val_accuracy: 0.5842\n",
            "Epoch 95/200\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.8510 - accuracy: 0.6919 - val_loss: 2.2952 - val_accuracy: 0.5982\n",
            "Epoch 96/200\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.8776 - accuracy: 0.6795 - val_loss: 2.3056 - val_accuracy: 0.5982\n",
            "Epoch 97/200\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.8604 - accuracy: 0.6884 - val_loss: 2.2516 - val_accuracy: 0.5944\n",
            "peak memory: 6528.12 MiB, increment: 2443.68 MiB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbAq_rEeVwuH"
      },
      "source": [
        "**Testing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ux3SAloSVyiG"
      },
      "source": [
        "start = time.time()\n",
        "\n",
        "if FLAGS_dataset_type in FLAGS_CNN_dataset_types:\n",
        "  %memit test_metrics = model.evaluate(X_test, Y_test)\n",
        "else:\n",
        "  %memit test_metrics = model.evaluate(test_gen)\n",
        "\n",
        "end = time.time()\n",
        "\n",
        "FLAGS_times['evaluating'] = end - start\n",
        "\n",
        "test_metrics_dict = {}\n",
        "for name, val in zip(model.metrics_names, test_metrics):\n",
        "  print('{}: {:0.4f}'.format(name, val), flush=True)\n",
        "  test_metrics_dict[name] = val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPh4ll_6VjsF"
      },
      "source": [
        "**Predicting**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzNtToDNVof8"
      },
      "source": [
        "start = time.time()\n",
        "\n",
        "if FLAGS_dataset_type in FLAGS_CNN_dataset_types:\n",
        "  %memit predicted = model.predict(X_test)\n",
        "else:\n",
        "  %memit predicted = model.predict(test_gen)\n",
        "\n",
        "end = time.time()\n",
        "\n",
        "FLAGS_times['predicting'] = end - start"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nV6j4cCVTS4"
      },
      "source": [
        "**Statistics**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZ4TtOgWVVoz"
      },
      "source": [
        "pred_y = predicted.argmax(axis=-1)\n",
        "\n",
        "cm = confusion_matrix(Y_test if FLAGS_dataset_type in FLAGS_CNN_dataset_types else Y_test.idxmax(axis=1), pred_y)\n",
        "print('Confusion matrix')\n",
        "print(cm)\n",
        "\n",
        "cr = classification_report(Y_test if FLAGS_dataset_type in FLAGS_CNN_dataset_types else Y_test.idxmax(axis=1), pred_y)\n",
        "print('\\n\\nClassification report')\n",
        "print(cr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2eBoEoru_b6"
      },
      "source": [
        "**Measure the final time**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMcOFoKlu_b8"
      },
      "source": [
        "final_time = time.time()\n",
        "FLAGS_times['elapsed'] = final_time - initial_time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27YlDA5XjE9c"
      },
      "source": [
        "**Store the results**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFpbdBcrjIvI"
      },
      "source": [
        "if FLAGS_store_results:\n",
        "  os.makedirs(FLAGS_results_directory, exist_ok=True)   \n",
        "\n",
        "  #\n",
        "  # Store the results\n",
        "  #\n",
        "\n",
        "  # History\n",
        "  fout = open('{}/history.yaml'.format(FLAGS_results_directory), 'w')\n",
        "  yl.dump(history.history, fout)\n",
        "  fout.close()\n",
        "\n",
        "  # Test metrics\n",
        "  fout = open('{}/test_metrics.yaml'.format(FLAGS_results_directory), 'w')\n",
        "  yl.dump(test_metrics_dict, fout)\n",
        "  fout.close()\n",
        "\n",
        "  # Dataset\n",
        "  if FLAGS_dataset_type in FLAGS_CNN_dataset_types:\n",
        "    np.savez_compressed('{}/y_train.npz'.format(FLAGS_results_directory), values=Y_train)\n",
        "    np.savez_compressed('{}/y_val.npz'.format(FLAGS_results_directory), values=Y_val)\n",
        "    np.savez_compressed('{}/y_test.npz'.format(FLAGS_results_directory), values=Y_test)\n",
        "  else:\n",
        "    Y_train.to_pickle('{}/train_graphs.pkl'.format(FLAGS_results_directory))\n",
        "    Y_val.to_pickle('{}/val_graphs.pkl'.format(FLAGS_results_directory))\n",
        "    Y_test.to_pickle('{}/test_graphs.pkl'.format(FLAGS_results_directory))\n",
        "\n",
        "  # Dataset Summary - GNN\n",
        "  if FLAGS_dataset_type not in FLAGS_CNN_dataset_types:\n",
        "    summary = pd.DataFrame(\n",
        "      [(g.number_of_nodes(), g.number_of_edges()) for g in X],\n",
        "      columns=['nodes', 'edges'],\n",
        "    )\n",
        "    fout = open('{}/summary.yaml'.format(FLAGS_results_directory), 'w')\n",
        "    yl.dump(summary.describe().to_dict(), fout)\n",
        "    fout.close()\n",
        "\n",
        "  # Predicted\n",
        "  np.savez_compressed('{}/predicted'.format(FLAGS_results_directory), values=predicted)\n",
        "\n",
        "  # Confusion matrix\n",
        "  np.savez_compressed('{}/confusion_matrix'.format(FLAGS_results_directory), values=cm)\n",
        "\n",
        "  # Classification report\n",
        "  fout = open('{}/classification_report.pk'.format(FLAGS_results_directory), 'wb')\n",
        "  pk.dump(cr, fout)\n",
        "  fout.close()\n",
        "\n",
        "  # Time\n",
        "  fout = open('{}/elapsed_time.yaml'.format(FLAGS_results_directory), 'w')\n",
        "  yl.dump(FLAGS_times, fout)\n",
        "  fout.close()\n",
        "\n",
        "  #\n",
        "  # Store the hadware specifications\n",
        "  #\n",
        "\n",
        "  spec, _ = execute(\"nvidia-smi\")\n",
        "  filename = '{}/nvidia-smi.txt'.format(FLAGS_results_directory)\n",
        "  save(filename, spec)\n",
        "\n",
        "  spec, _ = execute(\"cat /proc/cpuinfo\")\n",
        "  filename = '{}/cpuinfo.txt'.format(FLAGS_results_directory)\n",
        "  save(filename, spec)\n",
        "\n",
        "  spec, _ = execute(\"cat /proc/meminfo\")\n",
        "  filename = '{}/meminfo.txt'.format(FLAGS_results_directory)\n",
        "  save(filename, spec)  \n",
        "\n",
        "  #\n",
        "  # Store the model\n",
        "  #\n",
        "  if FLAGS_store_model:\n",
        "    directory = '{}/model'.format(FLAGS_results_directory)\n",
        "    os.makedirs(directory, exist_ok=True)\n",
        "    model.save(directory)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Md7qDdRHiubJ"
      },
      "source": [
        "**Free the memory**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCVYdx2ii1Px"
      },
      "source": [
        "if FLAGS_dataset_type not in FLAGS_CNN_dataset_types:\n",
        "  del X\n",
        "  del Y_train\n",
        "  del Y_val\n",
        "  del Y_test\n",
        "  del gen\n",
        "  del train_gen\n",
        "  del val_gen\n",
        "  del test_gen\n",
        "else:\n",
        "  del X_train\n",
        "  del X_val\n",
        "  del X_test\n",
        "  del Y_train\n",
        "  del Y_val\n",
        "  del Y_test\n",
        "\n",
        "del model\n",
        "del history\n",
        "del test_metrics\n",
        "del test_metrics_dict\n",
        "del predicted\n",
        "del cm\n",
        "del cr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQPhDT5CGIp-"
      },
      "source": [
        "**Flush Google Driver**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4qPqfYXetkJ"
      },
      "source": [
        "if FLAGS_store_results:\n",
        "  #\n",
        "  # Flush the Driver\n",
        "  #\n",
        "  drive.flush_and_unmount()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXVZV-rmifUX"
      },
      "source": [
        "**Finish**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jj_NvS_7Hv03"
      },
      "source": [
        "for key, value in FLAGS_times.items():\n",
        "    print(key, value)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}